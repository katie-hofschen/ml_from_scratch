{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "from activation_functions import sigmoid\n",
    "from metrics import accuracy\n",
    "from BaseRegression import BaseRegression\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Decision tree is a powerful algorithm that can fit complex data and perform both classification, regression, and multioutput tasks.\n",
    "\n",
    "Advantage of Decision Trees:\n",
    "* Make very few assumptions about the data.\n",
    "* Fairly intuitive and the decisions are easy to interpret. (white box model)\n",
    "* Feature scaling and centering is not necessary to obtain good results.\n",
    "\n",
    "Other noteworthy info:\n",
    "* Decision trees form the fundamental components of a RandomForest.\n",
    "* The CART algorithm (scikit-learn) produces only binary trees whereas ID3 for example allows nodes to have more than 2 children.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1060/1*H6thrs5CR_wdxQyMCwWawQ.png\" alt=\"Image of Decision tree\" style=\"background-color:white;\">\n",
    "\n",
    "### <span style=\"color:#217AB8\"> Making predictions</span> \n",
    "\n",
    "Starting at the root node and follow the conditions that apply to your current instance to the leaf. This will look like is the attribute x of your instance larger or smaller than 1. If yes follow the tree down the right path. If no go down the left. Once you reach a leaf node (aka does not have any child nodes) use this node's class to predict the class of your instance.\\\n",
    "It takes approximately $O(log_2(m))$ nodes to predict an instance's class. This is indipendent of the number of features so predictions are very fast even with large training sets.\n",
    "\n",
    "* *sample* how many training samples a node's condition applies to.\\\n",
    "* *value* how many training samples of each class a node applies to (eg. in node x: class a is represented 1, class b is represented 20 times)\n",
    "* *gini* measures the impurity of a node (pure when a node applies to only instances of one class)\n",
    "\n",
    "Gini impurity = $G_i = 1  - \\sum_{k=1}^n p_{i,k}^2$ where $p_{i,k}$ is the ratio of class k instances among the training instances in the ith node.\n",
    "\n",
    "### <span style=\"color:#217AB8\">CART classification and regression tree algorithm</span> \n",
    "A greedy algorithm meaning at every step from the beginning it tries to optimize the split rather than checking whether this improves impurity further down the line at lower levels.\\\n",
    "Therefore it does not guarantee an optimal solution.\\\n",
    "It is also an NP complete problem and requires O(exp(m)) time, making it hard to work with even small training sets. -> find reasonalbly good solutions.\n",
    "\n",
    "1. Split the training set into 2 using a single feature $k$ and a threshold $t_k$ (eg. petal length >= 1.3). Find the purest subsets for pairs (k, $t_k$) weighted by their size.\n",
    "\n",
    "&emsp;&emsp;&emsp;Minimize Cost function: $ J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$ where m is the number of instances in the subesets\n",
    "\n",
    "2. Continue this on the subsets recursively.\n",
    "3. Stop when max_depth is reached or if no split further reduces the gini impurity.\\\n",
    "Training complexity requires the comparison of all features (unless max_features is set) on all samples at each node.\\\n",
    "This brings the training compexity to $O(n * m * log(m))$\n",
    "\n",
    "If the tree is left unconstrained it will fit itself very closely to the training data and most likely overfitting.\\\n",
    "This is often described as a non-parametric model. In contrast, parametric models such as linear models have a pre-determined number of parameters, so their degrees of freedom are limited.\\\n",
    "(which in turn can lead to underfitting, especially when the data contains more complex patterns than the model is able to catch)\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:#217AB8\">Gini impurity or Entropy</span> \n",
    "Shannon's information theory: Entropy measures the average information content of a message:\\\n",
    "Entropy is 0 when all messages are identical.\\\n",
    "For example if in the case of decision tree the entropy is 0 then the node captures only one class.\n",
    "\n",
    "Entropy in the ith node: $H_i = \\sum_{k=1}^{n} p_{i,k}*log(p_{i,k})$ where $p_{i,k}\\not=0$\n",
    "\n",
    "[Article explaining entropy, information gain and gini](https://www.machinelearningnuggets.com/splitting-criteria-in-decision-trees/#:~:text=Entropy%20measures%20data%20points%27%20degree,ranges%20between%200%20and%201.&text=We%20can%20see%20that%20the,the%20data%20is%20perfectly%20randomized.\n",
    ")\n",
    "\n",
    "Example:\n",
    "\n",
    "Should you use Gini or Entropy?\n",
    "According to O'reilly \"Hands on Machine learning\" they lead to similar trees.\\\n",
    "\"The gini index seems to be slightly faster to compute so it is a good default.\\\n",
    "However when they differ the Gini impurity tends to isolate the most frequent class  in its own branch of the tree, while entropy tends to produce slightly more balanced trees.\" (Sebastian Raschka's analysis)\n",
    "\n",
    "### <span style=\"color:#217AB8\">Regularization</span> \n",
    "For example:\n",
    "* restrict the maximum depth of the tree\n",
    "* set the minumum number of samples a node must have before it can split\n",
    "* set the minumum number of samples a leaf must have\n",
    "* set the minumum fraction of all training data that a leaf must represent\n",
    "* restrict the maximum number of leaf nodes that can be determined\n",
    "* restrict the maximum number of features considered for splitting at a node\n",
    "\n",
    "You could also train the tree without restrictions and then prune the tree after the training.\n",
    "For example:\n",
    "Prune a node if all of its children are leaves and it provides no statistically significant improvement of purity.\\\n",
    "Using the chi-sqaure test with null hypothesis that the node increases purity. If you want to reject the null hypothesis with 95% confidence then the p-value should be over 0.05. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entropy(classes):\n",
    "    # Entropy measures data points' degree of impurity, uncertainty, or surprise. \n",
    "    # Range [0 and 1]: equals 1 when the data is perfectly randomized.\n",
    "    # Expected Value of surprise\n",
    "    probs = np.bincount(classes) / len(classes)\n",
    "    return -np.sum([px * np.log2(px) for px in probs if px > 0])\n",
    "\n",
    "def info_gain(labels, column):\n",
    "    # column: is the chosen attribute column of the dataset\n",
    "    # labels: the target array y\n",
    "    # The information gain of an attribute\n",
    "    # I(attribute a) = Entropy of dataset - sum over unique values v in a{( count(v)/len(y) * Entropy(Unique value in a))}\n",
    "    dataset_entropy = entropy(labels)\n",
    "    sum_val_entropies = 0\n",
    "    for val in np.unique(column):\n",
    "        value_ids = [id for id, elem in enumerate(column) if elem == val]\n",
    "        val_clss=[clss for id, clss in enumerate(labels) if id in value_ids]\n",
    "        sum_val_entropies += column.count(val) / len(column) * entropy(val_clss)\n",
    "    return dataset_entropy - sum_val_entropies\n",
    "\n",
    "def gini(classes):\n",
    "    #compute the gini impurity\n",
    "    probs = np.bincount(classes) / len(classes)\n",
    "    return 1 - np.sum(np.square(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6186951736180384"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clss = [1,1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,4,4,4,4,4,4,4,4,4]\n",
    "vls = ['b','b','b','b','b','b','c','c','c','b','b','b','b','a','a','a','b','b','b','b','a','a','a','a','a']\n",
    "\n",
    "info_gain(clss, vls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, samples=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.samples = samples\n",
    "        self.gini = None\n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split =min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features =n_features \n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # ensure that the number of features to use is never larger than the actual existing features.\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(self.n_features, X.shape[1])\n",
    "        # grow the tree with its choices and thresholds\n",
    "        self.root = self._grow_tree(X, y)\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # check for stopping criteria before continuing to grow the tree.\n",
    "        if (depth >= self.max_depth \n",
    "            or n_samples < self.min_samples_split\n",
    "            or n_labels == 1):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value, samples=n_samples)\n",
    "        \n",
    "        # if stopping criteria is not met countinue growing the tree.\n",
    "        feature_ids = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "        \n",
    "        # greedy search for best features and thresholds\n",
    "        best_feat, best_thresh = self._best_criteria(X,y, feature_ids)\n",
    "\n",
    "        # split based on best feature and threshold\n",
    "        left_ids, right_ids = self._split(X[:, best_feat], best_thresh)\n",
    "\n",
    "        # Continue growing the tree for the left and right children of current node\n",
    "        left  = self._grow_tree( X[left_ids, :], y[left_ids], depth+1)\n",
    "        right = self._grow_tree( X[right_ids, :], y[right_ids], depth+1)\n",
    "\n",
    "        return Node(best_feat, best_thresh, n_samples, left, right)\n",
    "\n",
    "\n",
    "    def _info_gain(self, labels, column, threshold):\n",
    "        # column: is the chosen attribute column of the dataset\n",
    "        # labels: the target array y\n",
    "        # The information gain of an attribute\n",
    "        # I(attribute a) = Entropy of dataset - sum over unique values v in a{( count(v)/len(y) * Entropy(Unique value in a))}\n",
    "        parent_entropy = entropy(labels)\n",
    "        left_ids, right_ids = self._split(column, threshold)\n",
    "        \n",
    "        if len(left_ids) == 0 or len(right_ids) == 0:\n",
    "            return 0\n",
    "        \n",
    "        left_entropy, right_entropy = entropy(labels[left_ids]), entropy(labels[right_ids])\n",
    "        child_entropy = (len(left_ids) / len(labels)) * left_entropy + (len(right_ids) / len(labels)) * right_entropy\n",
    "        return (parent_entropy - child_entropy)\n",
    "\n",
    "    def _split(self, col, split_thresh):\n",
    "        left_ids = np.argwhere(col <= split_thresh).flatten()\n",
    "        right_ids = np.argwhere(col > split_thresh).flatten()\n",
    "        return left_ids, right_ids\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        most_common = Counter(y).most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_ids):\n",
    "        # brute force search over all unique values of all relevant features\n",
    "        # Goal: find the features and splits that have the highest information gain\n",
    "        best_gain = -1\n",
    "        split_id, split_thresh = None, None\n",
    "        for feat_id in feat_ids:\n",
    "            X_col = X[:, feat_id]\n",
    "            threshold = np.unique(X_col)\n",
    "            for thresh in threshold:\n",
    "                gain = self._info_gain(y, X_col, thresh)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_id = feat_id\n",
    "                    split_thresh = thresh\n",
    "        return split_id, split_thresh\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # for each sample run through the tree and predict the class of its leaf node.\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        # with Recursion\n",
    "        # if we are at a leaf node return the most common label saved as the value of the node \n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        # if the feature f of our sample s is equal to or below the threshold go to the left child node\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        # else the the feature f is larger than the threshold and continue to the right child node\n",
    "        return self._traverse_tree(x, node.right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = datasets.load_breast_cancer()\n",
    "\n",
    "X = cancer_data.data\n",
    "# Target \"1\" : Benign, \"0\":Malignant\n",
    "y = cancer_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree(max_depth=10)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "predicted = tree.predict(X_test)\n",
    "accuracy = accuracy(predicted, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'decision_tree' parameter of plot_tree must be an instance of 'sklearn.tree._classes.DecisionTreeClassifier' or an instance of 'sklearn.tree._classes.DecisionTreeRegressor'. Got <__main__.DecisionTree object at 0x129f6b320> instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/katie/Coding/ml_from_scratch/DecisionTree.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/katie/Coding/ml_from_scratch/DecisionTree.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m label_names \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mMalignant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mBenign\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/katie/Coding/ml_from_scratch/DecisionTree.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_tree \u001b[39m# tree diagram\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/katie/Coding/ml_from_scratch/DecisionTree.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plot_tree(tree, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/katie/Coding/ml_from_scratch/DecisionTree.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m           feature_names \u001b[39m=\u001b[39;49m cancer_data\u001b[39m.\u001b[39;49mfeature_names, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/katie/Coding/ml_from_scratch/DecisionTree.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m           class_names \u001b[39m=\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39mMalignant\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mBenign\u001b[39;49m\u001b[39m\"\u001b[39;49m], \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/katie/Coding/ml_from_scratch/DecisionTree.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m           filled \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/katie/Coding/ml_from_scratch/DecisionTree.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m           rounded \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Coding/ml_from_scratch/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:204\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m to_ignore \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    202\u001b[0m params \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m to_ignore}\n\u001b[0;32m--> 204\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    205\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39;49mfunc\u001b[39m.\u001b[39;49m\u001b[39m__qualname__\u001b[39;49m\n\u001b[1;32m    206\u001b[0m )\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n",
      "File \u001b[0;32m~/Coding/ml_from_scratch/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[1;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     97\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'decision_tree' parameter of plot_tree must be an instance of 'sklearn.tree._classes.DecisionTreeClassifier' or an instance of 'sklearn.tree._classes.DecisionTreeRegressor'. Got <__main__.DecisionTree object at 0x129f6b320> instead."
     ]
    }
   ],
   "source": [
    "label_names = {0:\"Malignant\", 1:\"Benign\"}\n",
    "\n",
    "# To Do: See if you can learn something from scikit learn implementation for the visualization.\n",
    "\n",
    "def plot_tree(decision_tree, *, max_depth=None, feature_names=None, class_names=None, label=\"all\", filled=False, impurity=True, \n",
    "              node_ids=False, proportion=False, rounded=False, precision=3, ax=None, fontsize=None,):\n",
    "    \"\"\"Plot a decision tree.\n",
    "    The sample counts that are shown are weighted with any sample_weights that\n",
    "    might be present.\n",
    "\n",
    "    The visualization is fit automatically to the size of the axis.\n",
    "    Use the ``figsize`` or ``dpi`` arguments of ``plt.figure``  to control\n",
    "    the size of the rendering.\n",
    "\n",
    "    Read more in the :ref:`User Guide <tree>`.\n",
    "\n",
    "    .. versionadded:: 0.21\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    decision_tree : decision tree regressor or classifier\n",
    "        The decision tree to be plotted.\n",
    "\n",
    "    max_depth : int, default=None\n",
    "        The maximum depth of the representation. If None, the tree is fully\n",
    "        generated.\n",
    "\n",
    "    feature_names : array-like of str, default=None\n",
    "        Names of each of the features.\n",
    "        If None, generic names will be used (\"x[0]\", \"x[1]\", ...).\n",
    "\n",
    "    class_names : array-like of str or True, default=None\n",
    "        Names of each of the target classes in ascending numerical order.\n",
    "        Only relevant for classification and not supported for multi-output.\n",
    "        If ``True``, shows a symbolic representation of the class name.\n",
    "\n",
    "    label : {'all', 'root', 'none'}, default='all'\n",
    "        Whether to show informative labels for impurity, etc.\n",
    "        Options include 'all' to show at every node, 'root' to show only at\n",
    "        the top root node, or 'none' to not show at any node.\n",
    "\n",
    "    filled : bool, default=False\n",
    "        When set to ``True``, paint nodes to indicate majority class for\n",
    "        classification, extremity of values for regression, or purity of node\n",
    "        for multi-output.\n",
    "\n",
    "        impurity : bool, default=True\n",
    "        When set to ``True``, show the impurity at each node.\n",
    "\n",
    "    node_ids : bool, default=False\n",
    "        When set to ``True``, show the ID number on each node.\n",
    "\n",
    "    proportion : bool, default=False\n",
    "        When set to ``True``, change the display of 'values' and/or 'samples'\n",
    "        to be proportions and percentages respectively.\n",
    "\n",
    "    rounded : bool, default=False\n",
    "        When set to ``True``, draw node boxes with rounded corners and use\n",
    "        Helvetica fonts instead of Times-Roman.\n",
    "\n",
    "    precision : int, default=3\n",
    "        Number of digits of precision for floating point in the values of\n",
    "        impurity, threshold and value attributes of each node.\n",
    "\n",
    "    ax : matplotlib axis, default=None\n",
    "        Axes to plot to. If None, use current axis. Any previous content\n",
    "        is cleared.\n",
    "\n",
    "    fontsize : int, default=None\n",
    "        Size of text font. If None, determined automatically to fit figure.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    annotations : list of artists\n",
    "        List containing the artists for the annotation boxes making up the\n",
    "        tree.\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    check_is_fitted(decision_tree)\n",
    "\n",
    "    exporter = _MPLTreeExporter(max_depth=max_depth, feature_names=feature_names, class_names=class_names, label=label, filled=filled,\n",
    "                                impurity=impurity, node_ids=node_ids, proportion=proportion, rounded=rounded, precision=precision, fontsize=fontsize,)\n",
    "    return exporter.export(decision_tree, ax=ax)\n",
    "\n",
    "class _BaseTreeExporter:\n",
    "    def __init__(self, max_depth=None, feature_names=None, class_names=None, label=\"all\", filled=False, impurity=True,\n",
    "                node_ids=False, proportion=False, rounded=False, precision=3, fontsize=None,):\n",
    "        self.max_depth = max_depth\n",
    "        self.feature_names = feature_names\n",
    "        self.class_names = class_names\n",
    "        self.label = label\n",
    "        self.filled = filled\n",
    "        self.impurity = impurity\n",
    "        self.node_ids = node_ids\n",
    "        self.proportion = proportion\n",
    "        self.rounded = rounded\n",
    "        self.precision = precision\n",
    "        self.fontsize = fontsize\n",
    "\n",
    "    def get_color(self, value):\n",
    "        # Find the appropriate color & intensity for a node\n",
    "        if self.colors[\"bounds\"] is None:\n",
    "            # Classification tree\n",
    "            color = list(self.colors[\"rgb\"][np.argmax(value)])\n",
    "            sorted_values = sorted(value, reverse=True)\n",
    "            if len(sorted_values) == 1:\n",
    "                alpha = 0.0\n",
    "            else:\n",
    "                alpha = (sorted_values[0] - sorted_values[1]) / (1 - sorted_values[1])\n",
    "        else:\n",
    "            # Regression tree or multi-output\n",
    "            color = list(self.colors[\"rgb\"][0])\n",
    "            alpha = (value - self.colors[\"bounds\"][0]) / (\n",
    "                self.colors[\"bounds\"][1] - self.colors[\"bounds\"][0]\n",
    "            )\n",
    "        # compute the color as alpha against white\n",
    "        color = [int(round(alpha * c + (1 - alpha) * 255, 0)) for c in color]\n",
    "        # Return html color code in #RRGGBB format\n",
    "        return \"#%2x%2x%2x\" % tuple(color)\n",
    "    \n",
    "    def get_fill_color(self, tree, node_id):\n",
    "        # Fetch appropriate color for node\n",
    "        if \"rgb\" not in self.colors:\n",
    "            # Initialize colors and bounds if required\n",
    "            self.colors[\"rgb\"] = _color_brew(tree.n_classes[0])\n",
    "            if tree.n_outputs != 1:\n",
    "                # Find max and min impurities for multi-output\n",
    "                self.colors[\"bounds\"] = (np.min(-tree.impurity), np.max(-tree.impurity))\n",
    "            elif tree.n_classes[0] == 1 and len(np.unique(tree.value)) != 1:\n",
    "                # Find max and min values in leaf nodes for regression\n",
    "                self.colors[\"bounds\"] = (np.min(tree.value), np.max(tree.value))\n",
    "        if tree.n_outputs == 1:\n",
    "            node_val = tree.value[node_id][0, :] / tree.weighted_n_node_samples[node_id]\n",
    "            if tree.n_classes[0] == 1:\n",
    "                # Regression or degraded classification with single class\n",
    "                node_val = tree.value[node_id][0, :]\n",
    "                if isinstance(node_val, Iterable) and self.colors[\"bounds\"] is not None:\n",
    "                    # Only unpack the float only for the regression tree case.\n",
    "                    # Classification tree requires an Iterable in `get_color`.\n",
    "                    node_val = node_val.item()\n",
    "        else:\n",
    "            # If multi-output color node by impurity\n",
    "            node_val = -tree.impurity[node_id]\n",
    "        return self.get_color(node_val)\n",
    "\n",
    "\n",
    "    def node_to_str(self, tree, node_id, criterion):\n",
    "        # Generate the node content string\n",
    "        if tree.n_outputs == 1:\n",
    "            value = tree.value[node_id][0, :]\n",
    "        else:\n",
    "            value = tree.value[node_id]\n",
    "\n",
    "        # Should labels be shown?\n",
    "        labels = (self.label == \"root\" and node_id == 0) or self.label == \"all\"\n",
    "\n",
    "        characters = self.characters\n",
    "        node_string = characters[-1]\n",
    "\n",
    "        # Write node ID\n",
    "        if self.node_ids:\n",
    "            if labels:\n",
    "                node_string += \"node \"\n",
    "            node_string += characters[0] + str(node_id) + characters[4]\n",
    "\n",
    "        # Write decision criteria\n",
    "        if tree.children_left[node_id] != _tree.TREE_LEAF:\n",
    "            # Always write node decision criteria, except for leaves\n",
    "            if self.feature_names is not None:\n",
    "                feature = self.feature_names[tree.feature[node_id]]\n",
    "            else:\n",
    "                feature = \"x%s%s%s\" % (\n",
    "                    characters[1],\n",
    "                    tree.feature[node_id],\n",
    "                    characters[2],\n",
    "                )\n",
    "            node_string += \"%s %s %s%s\" % (\n",
    "                feature,\n",
    "                characters[3],\n",
    "                round(tree.threshold[node_id], self.precision),\n",
    "                characters[4],\n",
    "            )\n",
    "\n",
    "        # Write impurity\n",
    "        if self.impurity:\n",
    "            if isinstance(criterion, _criterion.FriedmanMSE):\n",
    "                criterion = \"friedman_mse\"\n",
    "            elif isinstance(criterion, _criterion.MSE) or criterion == \"squared_error\":\n",
    "                criterion = \"squared_error\"\n",
    "            elif not isinstance(criterion, str):\n",
    "                criterion = \"impurity\"\n",
    "            if labels:\n",
    "                node_string += \"%s = \" % criterion\n",
    "            node_string += (\n",
    "                str(round(tree.impurity[node_id], self.precision)) + characters[4]\n",
    "            )\n",
    "\n",
    "            # Write impurity\n",
    "        if self.impurity:\n",
    "            if isinstance(criterion, _criterion.FriedmanMSE):\n",
    "                criterion = \"friedman_mse\"\n",
    "            elif isinstance(criterion, _criterion.MSE) or criterion == \"squared_error\":\n",
    "                criterion = \"squared_error\"\n",
    "            elif not isinstance(criterion, str):\n",
    "                criterion = \"impurity\"\n",
    "            if labels:\n",
    "                node_string += \"%s = \" % criterion\n",
    "            node_string += (\n",
    "                str(round(tree.impurity[node_id], self.precision)) + characters[4]\n",
    "            )\n",
    "\n",
    "        # Write node sample count\n",
    "        if labels:\n",
    "            node_string += \"samples = \"\n",
    "        if self.proportion:\n",
    "            percent = (\n",
    "                100.0 * tree.n_node_samples[node_id] / float(tree.n_node_samples[0])\n",
    "            )\n",
    "            node_string += str(round(percent, 1)) + \"%\" + characters[4]\n",
    "        else:\n",
    "            node_string += str(tree.n_node_samples[node_id]) + characters[4]\n",
    "\n",
    "\n",
    "# Write node class distribution / regression value\n",
    "        if self.proportion and tree.n_classes[0] != 1:\n",
    "            # For classification this will show the proportion of samples\n",
    "            value = value / tree.weighted_n_node_samples[node_id]\n",
    "        if labels:\n",
    "            node_string += \"value = \"\n",
    "        if tree.n_classes[0] == 1:\n",
    "            # Regression\n",
    "            value_text = np.around(value, self.precision)\n",
    "        elif self.proportion:\n",
    "            # Classification\n",
    "            value_text = np.around(value, self.precision)\n",
    "        elif np.all(np.equal(np.mod(value, 1), 0)):\n",
    "            # Classification without floating-point weights\n",
    "            value_text = value.astype(int)\n",
    "        else:\n",
    "            # Classification with floating-point weights\n",
    "            value_text = np.around(value, self.precision)\n",
    "        # Strip whitespace\n",
    "        value_text = str(value_text.astype(\"S32\")).replace(\"b'\", \"'\")\n",
    "        value_text = value_text.replace(\"' '\", \", \").replace(\"'\", \"\")\n",
    "        if tree.n_classes[0] == 1 and tree.n_outputs == 1:\n",
    "            value_text = value_text.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "        value_text = value_text.replace(\"\\n \", characters[4])\n",
    "        node_string += value_text + characters[4]\n",
    "\n",
    " # Write node majority class\n",
    "        if (\n",
    "            self.class_names is not None\n",
    "            and tree.n_classes[0] != 1\n",
    "            and tree.n_outputs == 1\n",
    "        ):\n",
    "            # Only done for single-output classification trees\n",
    "            if labels:\n",
    "                node_string += \"class = \"\n",
    "            if self.class_names is not True:\n",
    "                class_name = self.class_names[np.argmax(value)]\n",
    "            else:\n",
    "                class_name = \"y%s%s%s\" % (\n",
    "                    characters[1],\n",
    "                    np.argmax(value),\n",
    "                    characters[2],\n",
    "                )\n",
    "            node_string += class_name\n",
    "\n",
    "        # Clean up any trailing newlines\n",
    "        if node_string.endswith(characters[4]):\n",
    "            node_string = node_string[: -len(characters[4])]\n",
    "\n",
    "        return node_string + characters[5]\n",
    "\n",
    "\n",
    "\n",
    "class _DOTTreeExporter(_BaseTreeExporter):\n",
    "    def __init__( self, out_file=SENTINEL, max_depth=None, feature_names=None, class_names=None, label=\"all\", filled=False,\n",
    "                 leaves_parallel=False, impurity=True, node_ids=False, proportion=False, rotate=False, rounded=False,\n",
    "                 special_characters=False, precision=3, fontname=\"helvetica\",):\n",
    "        super().__init__(\n",
    "            max_depth=max_depth,\n",
    "            feature_names=feature_names,\n",
    "            class_names=class_names,\n",
    "            label=label,\n",
    "            filled=filled,\n",
    "            impurity=impurity,\n",
    "            node_ids=node_ids,\n",
    "            proportion=proportion,\n",
    "            rounded=rounded,\n",
    "            precision=precision,\n",
    "        )\n",
    "        self.leaves_parallel = leaves_parallel\n",
    "        self.out_file = out_file\n",
    "        self.special_characters = special_characters\n",
    "        self.fontname = fontname\n",
    "        self.rotate = rotate\n",
    "\n",
    "        # PostScript compatibility for special characters\n",
    "        if special_characters:\n",
    "            self.characters = [\"&#35;\", \"<SUB>\", \"</SUB>\", \"&le;\", \"<br/>\", \">\", \"<\"]\n",
    "        else:\n",
    "            self.characters = [\"#\", \"[\", \"]\", \"<=\", \"\\\\n\", '\"', '\"']\n",
    "\n",
    "        # The depth of each node for plotting with 'leaf' option\n",
    "        self.ranks = {\"leaves\": []}\n",
    "        # The colors to render each node with\n",
    "        self.colors = {\"bounds\": None}\n",
    "\n",
    "        def export(self, decision_tree):\n",
    "        # Check length of feature_names before getting into the tree node\n",
    "        # Raise error if length of feature_names does not match\n",
    "        # n_features_in_ in the decision_tree\n",
    "        if self.feature_names is not None:\n",
    "            if len(self.feature_names) != decision_tree.n_features_in_:\n",
    "                raise ValueError(\n",
    "                    \"Length of feature_names, %d does not match number of features, %d\"\n",
    "                    % (len(self.feature_names), decision_tree.n_features_in_)\n",
    "                )\n",
    "        # each part writes to out_file\n",
    "        self.head()\n",
    "        # Now recurse the tree and add node & edge attributes\n",
    "        if isinstance(decision_tree, _tree.Tree):\n",
    "            self.recurse(decision_tree, 0, criterion=\"impurity\")\n",
    "        else:\n",
    "            self.recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)\n",
    "\n",
    "        self.tail()\n",
    "\n",
    "    def tail(self):\n",
    "        # If required, draw leaf nodes at same depth as each other\n",
    "        if self.leaves_parallel:\n",
    "            for rank in sorted(self.ranks):\n",
    "                self.out_file.write(\n",
    "                    \"{rank=same ; \" + \"; \".join(r for r in self.ranks[rank]) + \"} ;\\n\"\n",
    "                )\n",
    "        self.out_file.write(\"}\")\n",
    "\n",
    "    def head(self):\n",
    "        self.out_file.write(\"digraph Tree {\\n\")\n",
    "\n",
    "        # Specify node aesthetics\n",
    "        self.out_file.write(\"node [shape=box\")\n",
    "        rounded_filled = []\n",
    "        if self.filled:\n",
    "            rounded_filled.append(\"filled\")\n",
    "        if self.rounded:\n",
    "            rounded_filled.append(\"rounded\")\n",
    "        if len(rounded_filled) > 0:\n",
    "            self.out_file.write(\n",
    "                ', style=\"%s\", color=\"black\"' % \", \".join(rounded_filled)\n",
    "            )\n",
    "        self.out_file.write(', fontname=\"%s\"' % self.fontname)\n",
    "        self.out_file.write(\"] ;\\n\")\n",
    "\n",
    "        # Specify graph & edge aesthetics\n",
    "        if self.leaves_parallel:\n",
    "            self.out_file.write(\"graph [ranksep=equally, splines=polyline] ;\\n\")\n",
    "\n",
    "        self.out_file.write('edge [fontname=\"%s\"] ;\\n' % self.fontname)\n",
    "\n",
    "        if self.rotate:\n",
    "            self.out_file.write(\"rankdir=LR ;\\n\")\n",
    "\n",
    "\n",
    "    def recurse(self, tree, node_id, criterion, parent=None, depth=0):\n",
    "        if node_id == _tree.TREE_LEAF:\n",
    "            raise ValueError(\"Invalid node_id %s\" % _tree.TREE_LEAF)\n",
    "\n",
    "        left_child = tree.children_left[node_id]\n",
    "        right_child = tree.children_right[node_id]\n",
    "\n",
    "        # Add node with description\n",
    "        if self.max_depth is None or depth <= self.max_depth:\n",
    "            # Collect ranks for 'leaf' option in plot_options\n",
    "            if left_child == _tree.TREE_LEAF:\n",
    "                self.ranks[\"leaves\"].append(str(node_id))\n",
    "            elif str(depth) not in self.ranks:\n",
    "                self.ranks[str(depth)] = [str(node_id)]\n",
    "            else:\n",
    "                self.ranks[str(depth)].append(str(node_id))\n",
    "\n",
    "            self.out_file.write(\n",
    "                \"%d [label=%s\" % (node_id, self.node_to_str(tree, node_id, criterion))\n",
    "            )\n",
    "\n",
    "            if self.filled:\n",
    "                self.out_file.write(\n",
    "                    ', fillcolor=\"%s\"' % self.get_fill_color(tree, node_id)\n",
    "                )\n",
    "            self.out_file.write(\"] ;\\n\")\n",
    "        \n",
    "            if parent is not None:\n",
    "                # Add edge to parent\n",
    "                self.out_file.write(\"%d -> %d\" % (parent, node_id))\n",
    "                if parent == 0:\n",
    "                    # Draw True/False labels if parent is root node\n",
    "                    angles = np.array([45, -45]) * ((self.rotate - 0.5) * -2)\n",
    "                    self.out_file.write(\" [labeldistance=2.5, labelangle=\")\n",
    "                    if node_id == 1:\n",
    "                        self.out_file.write('%d, headlabel=\"True\"]' % angles[0])\n",
    "                    else:\n",
    "                        self.out_file.write('%d, headlabel=\"False\"]' % angles[1])\n",
    "                self.out_file.write(\" ;\\n\")\n",
    "\n",
    "            if left_child != _tree.TREE_LEAF:\n",
    "                self.recurse(\n",
    "                    tree,\n",
    "                    left_child,\n",
    "                    criterion=criterion,\n",
    "                    parent=node_id,\n",
    "                    depth=depth + 1,\n",
    "                )\n",
    "                self.recurse(\n",
    "                    tree,\n",
    "                    right_child,\n",
    "                    criterion=criterion,\n",
    "                    parent=node_id,\n",
    "                    depth=depth + 1,\n",
    "                )\n",
    "        else:\n",
    "            self.ranks[\"leaves\"].append(str(node_id))\n",
    "\n",
    "            self.out_file.write('%d [label=\"(...)\"' % node_id)\n",
    "            if self.filled:\n",
    "                # color cropped nodes grey\n",
    "                self.out_file.write(', fillcolor=\"#C0C0C0\"')\n",
    "            self.out_file.write(\"] ;\\n\" % node_id)\n",
    "\n",
    "            if parent is not None:\n",
    "                # Add edge to parent\n",
    "                self.out_file.write(\"%d -> %d ;\\n\" % (parent, node_id))\n",
    "\n",
    "            \n",
    "class _MPLTreeExporter(_BaseTreeExporter):\n",
    "    def __init__(self, max_depth=None, feature_names=None, class_names=None, label=\"all\", filled=False, \n",
    "                 impurity=True, node_ids=False, proportion=False, rounded=False, precision=3, fontsize=None,):\n",
    "        super().__init__(\n",
    "            max_depth=max_depth,\n",
    "            feature_names=feature_names,\n",
    "            class_names=class_names,\n",
    "            label=label,\n",
    "            filled=filled,\n",
    "            impurity=impurity,\n",
    "            node_ids=node_ids,\n",
    "            proportion=proportion,\n",
    "            rounded=rounded,\n",
    "            precision=precision,\n",
    "        )\n",
    "        self.fontsize = fontsize\n",
    "\n",
    "        # The depth of each node for plotting with 'leaf' option\n",
    "        self.ranks = {\"leaves\": []}\n",
    "        # The colors to render each node with\n",
    "        self.colors = {\"bounds\": None}\n",
    "\n",
    "        self.characters = [\"#\", \"[\", \"]\", \"<=\", \"\\n\", \"\", \"\"]\n",
    "        self.bbox_args = dict()\n",
    "        if self.rounded:\n",
    "            self.bbox_args[\"boxstyle\"] = \"round\"\n",
    "\n",
    "        self.arrow_args = dict(arrowstyle=\"<-\")\n",
    "\n",
    "    def _make_tree(self, node_id, et, criterion, depth=0):\n",
    "        # traverses _tree.Tree recursively, builds intermediate\n",
    "        # \"_reingold_tilford.Tree\" object\n",
    "        name = self.node_to_str(et, node_id, criterion=criterion)\n",
    "        if et.children_left[node_id] != _tree.TREE_LEAF and (\n",
    "            self.max_depth is None or depth <= self.max_depth\n",
    "        ):\n",
    "            children = [\n",
    "                self._make_tree(\n",
    "                    et.children_left[node_id], et, criterion, depth=depth + 1\n",
    "                ),\n",
    "                self._make_tree(\n",
    "                    et.children_right[node_id], et, criterion, depth=depth + 1\n",
    "                ),\n",
    "            ]\n",
    "        else:\n",
    "            return Tree(name, node_id)\n",
    "        return Tree(name, node_id, *children)\n",
    "\n",
    "    def export(self, decision_tree, ax=None):\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib.text import Annotation\n",
    "\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        ax.clear()\n",
    "        ax.set_axis_off()\n",
    "        my_tree = self._make_tree(0, decision_tree.tree_, decision_tree.criterion)\n",
    "        draw_tree = buchheim(my_tree)\n",
    "\n",
    "        # important to make sure we're still\n",
    "        # inside the axis after drawing the box\n",
    "        # this makes sense because the width of a box\n",
    "        # is about the same as the distance between boxes\n",
    "        max_x, max_y = draw_tree.max_extents() + 1\n",
    "        ax_width = ax.get_window_extent().width\n",
    "        ax_height = ax.get_window_extent().height\n",
    "\n",
    "        scale_x = ax_width / max_x\n",
    "        scale_y = ax_height / max_y\n",
    "        self.recurse(draw_tree, decision_tree.tree_, ax, max_x, max_y)\n",
    "\n",
    "        anns = [ann for ann in ax.get_children() if isinstance(ann, Annotation)]\n",
    "\n",
    "        # update sizes of all bboxes\n",
    "        renderer = ax.figure.canvas.get_renderer()\n",
    "        for ann in anns:\n",
    "            ann.update_bbox_position_size(renderer)\n",
    "\n",
    "        if self.fontsize is None:\n",
    "            # get figure to data transform\n",
    "            # adjust fontsize to avoid overlap\n",
    "            # get max box width and height\n",
    "            extents = [ann.get_bbox_patch().get_window_extent() for ann in anns]\n",
    "            max_width = max([extent.width for extent in extents])\n",
    "            max_height = max([extent.height for extent in extents])\n",
    "            # width should be around scale_x in axis coordinates\n",
    "            size = anns[0].get_fontsize() * min(\n",
    "                scale_x / max_width, scale_y / max_height\n",
    "            )\n",
    "            for ann in anns:\n",
    "                ann.set_fontsize(size)\n",
    "\n",
    "        return anns\n",
    "\n",
    "    def recurse(self, node, tree, ax, max_x, max_y, depth=0):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        kwargs = dict(\n",
    "            bbox=self.bbox_args.copy(),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            zorder=100 - 10 * depth,\n",
    "            xycoords=\"axes fraction\",\n",
    "            arrowprops=self.arrow_args.copy(),\n",
    "        )\n",
    "\n",
    "        kwargs[\"arrowprops\"][\"edgecolor\"] = plt.rcParams[\"text.color\"]\n",
    "\n",
    "        if self.fontsize is not None:\n",
    "            kwargs[\"fontsize\"] = self.fontsize\n",
    "\n",
    "        # offset things by .5 to center them in plot\n",
    "        xy = ((node.x + 0.5) / max_x, (max_y - node.y - 0.5) / max_y)\n",
    "\n",
    "        if self.max_depth is None or depth <= self.max_depth:\n",
    "            if self.filled:\n",
    "                kwargs[\"bbox\"][\"fc\"] = self.get_fill_color(tree, node.tree.node_id)\n",
    "            else:\n",
    "                kwargs[\"bbox\"][\"fc\"] = ax.get_facecolor()\n",
    "\n",
    "            if node.parent is None:\n",
    "                # root\n",
    "                ax.annotate(node.tree.label, xy, **kwargs)\n",
    "            else:\n",
    "                xy_parent = (\n",
    "                    (node.parent.x + 0.5) / max_x,\n",
    "                    (max_y - node.parent.y - 0.5) / max_y,\n",
    "                )\n",
    "                ax.annotate(node.tree.label, xy_parent, xy, **kwargs)\n",
    "            for child in node.children:\n",
    "                self.recurse(child, tree, ax, max_x, max_y, depth=depth + 1)\n",
    "\n",
    "        else:\n",
    "            xy_parent = (\n",
    "                (node.parent.x + 0.5) / max_x,\n",
    "                (max_y - node.parent.y - 0.5) / max_y,\n",
    "            )\n",
    "            kwargs[\"bbox\"][\"fc\"] = \"grey\"\n",
    "            ax.annotate(\"\\n  (...)  \\n\", xy_parent, xy, **kwargs)\n",
    "\n",
    "\n",
    "@validate_params(\n",
    "    {\n",
    "        \"decision_tree\": \"no_validation\",\n",
    "        \"out_file\": [str, None, HasMethods(\"write\")],\n",
    "        \"max_depth\": [Interval(Integral, 0, None, closed=\"left\"), None],\n",
    "        \"feature_names\": [\"array-like\", None],\n",
    "        \"class_names\": [\"array-like\", \"boolean\", None],\n",
    "        \"label\": [StrOptions({\"all\", \"root\", \"none\"})],\n",
    "        \"filled\": [\"boolean\"],\n",
    "        \"leaves_parallel\": [\"boolean\"],\n",
    "        \"impurity\": [\"boolean\"],\n",
    "        \"node_ids\": [\"boolean\"],\n",
    "        \"proportion\": [\"boolean\"],\n",
    "        \"rotate\": [\"boolean\"],\n",
    "        \"rounded\": [\"boolean\"],\n",
    "        \"special_characters\": [\"boolean\"],\n",
    "        \"precision\": [Interval(Integral, 0, None, closed=\"left\"), None],\n",
    "        \"fontname\": [str],\n",
    "    },\n",
    "    prefer_skip_nested_validation=True,\n",
    ")\n",
    "def export_graphviz(\n",
    "    decision_tree,\n",
    "    out_file=None,\n",
    "    *,\n",
    "    max_depth=None,\n",
    "    feature_names=None,\n",
    "    class_names=None,\n",
    "    label=\"all\",\n",
    "    filled=False,\n",
    "    leaves_parallel=False,\n",
    "    impurity=True,\n",
    "    node_ids=False,\n",
    "    proportion=False,\n",
    "    rotate=False,\n",
    "    rounded=False,\n",
    "    special_characters=False,\n",
    "    precision=3,\n",
    "    fontname=\"helvetica\",\n",
    "):\n",
    "    \"\"\"Export a decision tree in DOT format.\n",
    "\n",
    "    This function generates a GraphViz representation of the decision tree,\n",
    "    which is then written into `out_file`. Once exported, graphical renderings\n",
    "    can be generated using, for example::\n",
    "\n",
    "        $ dot -Tps tree.dot -o tree.ps      (PostScript format)\n",
    "        $ dot -Tpng tree.dot -o tree.png    (PNG format)\n",
    "\n",
    "    The sample counts that are shown are weighted with any sample_weights that\n",
    "    might be present.\n",
    "\n",
    "    Read more in the :ref:`User Guide <tree>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    decision_tree : object\n",
    "        The decision tree estimator to be exported to GraphViz.\n",
    "\n",
    "    out_file : object or str, default=None\n",
    "        Handle or name of the output file. If ``None``, the result is\n",
    "        returned as a string.\n",
    "\n",
    "        .. versionchanged:: 0.20\n",
    "            Default of out_file changed from \"tree.dot\" to None.\n",
    "\n",
    "    max_depth : int, default=None\n",
    "        The maximum depth of the representation. If None, the tree is fully\n",
    "        generated.\n",
    "\n",
    "    feature_names : array-like of shape (n_features,), default=None\n",
    "        An array containing the feature names.\n",
    "        If None, generic names will be used (\"x[0]\", \"x[1]\", ...).\n",
    "\n",
    "    class_names : array-like of shape (n_classes,) or bool, default=None\n",
    "        Names of each of the target classes in ascending numerical order.\n",
    "        Only relevant for classification and not supported for multi-output.\n",
    "        If ``True``, shows a symbolic representation of the class name.\n",
    "\n",
    "    label : {'all', 'root', 'none'}, default='all'\n",
    "        Whether to show informative labels for impurity, etc.\n",
    "        Options include 'all' to show at every node, 'root' to show only at\n",
    "        the top root node, or 'none' to not show at any node.\n",
    "\n",
    "    filled : bool, default=False\n",
    "        When set to ``True``, paint nodes to indicate majority class for\n",
    "        classification, extremity of values for regression, or purity of node\n",
    "        for multi-output.\n",
    "\n",
    "    leaves_parallel : bool, default=False\n",
    "        When set to ``True``, draw all leaf nodes at the bottom of the tree.\n",
    "\n",
    "    impurity : bool, default=True\n",
    "        When set to ``True``, show the impurity at each node.\n",
    "\n",
    "    node_ids : bool, default=False\n",
    "        When set to ``True``, show the ID number on each node.\n",
    "\n",
    "    proportion : bool, default=False\n",
    "        When set to ``True``, change the display of 'values' and/or 'samples'\n",
    "        to be proportions and percentages respectively.\n",
    "\n",
    "    rotate : bool, default=False\n",
    "    When set to ``True``, orient tree left to right rather than top-down.\n",
    "\n",
    "    rounded : bool, default=False\n",
    "        When set to ``True``, draw node boxes with rounded corners.\n",
    "\n",
    "    special_characters : bool, default=False\n",
    "        When set to ``False``, ignore special characters for PostScript\n",
    "        compatibility.\n",
    "\n",
    "    precision : int, default=3\n",
    "        Number of digits of precision for floating point in the values of\n",
    "        impurity, threshold and value attributes of each node.\n",
    "\n",
    "    fontname : str, default='helvetica'\n",
    "        Name of font used to render text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dot_data : str\n",
    "        String representation of the input tree in GraphViz dot format.\n",
    "        Only returned if ``out_file`` is None.\n",
    "\n",
    "        .. versionadded:: 0.18\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn import tree\n",
    "\n",
    "    >>> clf = tree.DecisionTreeClassifier()\n",
    "    >>> iris = load_iris()\n",
    "    >>> clf = clf.fit(iris.data, iris.target)\n",
    "    >>> tree.export_graphviz(clf)\n",
    "    'digraph Tree {...\n",
    "    \"\"\"\n",
    "    if feature_names is not None:\n",
    "        feature_names = check_array(\n",
    "            feature_names, ensure_2d=False, dtype=None, ensure_min_samples=0\n",
    "        )\n",
    "    if class_names is not None and not isinstance(class_names, bool):\n",
    "        class_names = check_array(\n",
    "            class_names, ensure_2d=False, dtype=None, ensure_min_samples=0\n",
    "        )\n",
    "\n",
    "    check_is_fitted(decision_tree)\n",
    "    own_file = False\n",
    "    return_string = False\n",
    "    try:\n",
    "        if isinstance(out_file, str):\n",
    "            out_file = open(out_file, \"w\", encoding=\"utf-8\")\n",
    "            own_file = True\n",
    "\n",
    "        if out_file is None:\n",
    "            return_string = True\n",
    "            out_file = StringIO()\n",
    "            exporter = _DOTTreeExporter(\n",
    "            out_file=out_file,\n",
    "            max_depth=max_depth,\n",
    "            feature_names=feature_names,\n",
    "            class_names=class_names,\n",
    "            label=label,\n",
    "            filled=filled,\n",
    "            leaves_parallel=leaves_parallel,\n",
    "            impurity=impurity,\n",
    "            node_ids=node_ids,\n",
    "            proportion=proportion,\n",
    "            rotate=rotate,\n",
    "            rounded=rounded,\n",
    "            special_characters=special_characters,\n",
    "            precision=precision,\n",
    "            fontname=fontname,\n",
    "        )\n",
    "        exporter.export(decision_tree)\n",
    "\n",
    "        if return_string:\n",
    "            return exporter.out_file.getvalue()\n",
    "\n",
    "    finally:\n",
    "        if own_file:\n",
    "            out_file.close()\n",
    "\n",
    "    def _compute_depth(tree, node):\n",
    "        \"\"\"\n",
    "        Returns the depth of the subtree rooted in node.\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "    def compute_depth_(\n",
    "        current_node, current_depth, children_left, children_right, depths\n",
    "    ):\n",
    "        depths += [current_depth]\n",
    "        left = children_left[current_node]\n",
    "        right = children_right[current_node]\n",
    "        if left != -1 and right != -1:\n",
    "            compute_depth_(\n",
    "                left, current_depth + 1, children_left, children_right, depths\n",
    "            )\n",
    "            compute_depth_(\n",
    "                right, current_depth + 1, children_left, children_right, depths\n",
    "            )\n",
    "\n",
    "    depths = []\n",
    "    compute_depth_(node, 1, tree.children_left, tree.children_right, depths)\n",
    "    return max(depths)\n",
    "@validate_params(\n",
    "    {\n",
    "        \"decision_tree\": [DecisionTreeClassifier, DecisionTreeRegressor],\n",
    "        \"feature_names\": [\"array-like\", None],\n",
    "        \"class_names\": [\"array-like\", None],\n",
    "        \"max_depth\": [Interval(Integral, 0, None, closed=\"left\"), None],\n",
    "        \"spacing\": [Interval(Integral, 1, None, closed=\"left\"), None],\n",
    "        \"decimals\": [Interval(Integral, 0, None, closed=\"left\"), None],\n",
    "        \"show_weights\": [\"boolean\"],\n",
    "    },\n",
    "    prefer_skip_nested_validation=True,\n",
    ")\n",
    "\n",
    "def export_text(\n",
    "    decision_tree,\n",
    "    *,\n",
    "    feature_names=None,\n",
    "    class_names=None,\n",
    "    max_depth=10,\n",
    "    spacing=3,\n",
    "    decimals=2,\n",
    "    show_weights=False,\n",
    "):\n",
    "    \"\"\"Build a text report showing the rules of a decision tree.\n",
    "\n",
    "    Note that backwards compatibility may not be supported.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    decision_tree : object\n",
    "        The decision tree estimator to be exported.\n",
    "        It can be an instance ofDecisionTreeClassifier or DecisionTreeRegressor.\n",
    "\n",
    "    feature_names : array-like of shape (n_features,), default=None\n",
    "        An array containing the feature names.\n",
    "        If None generic names will be used (\"feature_0\", \"feature_1\", ...).\n",
    "\n",
    "    class_names : array-like of shape (n_classes,), default=None\n",
    "        Names of each of the target classes in ascending numerical order.\n",
    "        Only relevant for classification and not supported for multi-output.\n",
    "\n",
    "        - if `None`, the class names are delegated to `decision_tree.classes_`;\n",
    "        - otherwise, `class_names` will be used as class names instead of\n",
    "          `decision_tree.classes_`. The length of `class_names` must match\n",
    "          the length of `decision_tree.classes_`.\n",
    "\n",
    "        .. versionadded:: 1.3\n",
    "\n",
    "    max_depth : int, default=10\n",
    "        Only the first max_depth levels of the tree are exported.\n",
    "        Truncated branches will be marked with \"...\".\n",
    "\n",
    "    spacing : int, default=3\n",
    "        Number of spaces between edges. The higher it is, the wider the result.\n",
    "\n",
    "    decimals : int, default=2\n",
    "        Number of decimal digits to display.\n",
    "\n",
    "    show_weights : bool, default=False\n",
    "        If true the classification weights will be exported on each leaf.\n",
    "        The classification weights are the number of samples each class.\n",
    "        Returns\n",
    "    -------\n",
    "    report : str\n",
    "        Text summary of all the rules in the decision tree.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.tree import DecisionTreeClassifier\n",
    "    >>> from sklearn.tree import export_text\n",
    "    >>> iris = load_iris()\n",
    "    >>> X = iris['data']\n",
    "    >>> y = iris['target']\n",
    "    >>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "    >>> decision_tree = decision_tree.fit(X, y)\n",
    "    >>> r = export_text(decision_tree, feature_names=iris['feature_names'])\n",
    "    >>> print(r)\n",
    "    |--- petal width (cm) <= 0.80\n",
    "    |   |--- class: 0\n",
    "    |--- petal width (cm) >  0.80\n",
    "    |   |--- petal width (cm) <= 1.75\n",
    "    |   |   |--- class: 1\n",
    "    |   |--- petal width (cm) >  1.75\n",
    "    |   |   |--- class: 2\n",
    "    \"\"\"\n",
    "\n",
    "    if feature_names is not None:\n",
    "        feature_names = check_array(\n",
    "            feature_names, ensure_2d=False, dtype=None, ensure_min_samples=0\n",
    "        )\n",
    "    if class_names is not None:\n",
    "        class_names = check_array(\n",
    "            class_names, ensure_2d=False, dtype=None, ensure_min_samples=0\n",
    "        )\n",
    "\n",
    "    check_is_fitted(decision_tree)\n",
    "    tree_ = decision_tree.tree_\n",
    "    if is_classifier(decision_tree):\n",
    "        if class_names is None:\n",
    "            class_names = decision_tree.classes_\n",
    "        elif len(class_names) != len(decision_tree.classes_):\n",
    "            raise ValueError(\n",
    "                \"When `class_names` is an array, it should contain as\"\n",
    "                \" many items as `decision_tree.classes_`. Got\"\n",
    "                f\" {len(class_names)} while the tree was fitted with\"\n",
    "                f\" {len(decision_tree.classes_)} classes.\"\n",
    "            )\n",
    "    right_child_fmt = \"{} {} <= {}\\n\"\n",
    "    left_child_fmt = \"{} {} >  {}\\n\"\n",
    "    truncation_fmt = \"{} {}\\n\"\n",
    "\n",
    "    if feature_names is not None and len(feature_names) != tree_.n_features:\n",
    "        raise ValueError(\n",
    "            \"feature_names must contain %d elements, got %d\"\n",
    "            % (tree_.n_features, len(feature_names))\n",
    "        )\n",
    "    if isinstance(decision_tree, DecisionTreeClassifier):\n",
    "        value_fmt = \"{}{} weights: {}\\n\"\n",
    "        if not show_weights:\n",
    "            value_fmt = \"{}{}{}\\n\"\n",
    "    else:\n",
    "        value_fmt = \"{}{} value: {}\\n\"\n",
    "\n",
    "    if feature_names is not None:\n",
    "        feature_names_ = [\n",
    "            feature_names[i] if i != _tree.TREE_UNDEFINED else None\n",
    "            for i in tree_.feature\n",
    "        ]\n",
    "    else:\n",
    "        feature_names_ = [\"feature_{}\".format(i) for i in tree_.feature]\n",
    "\n",
    "    export_text.report = \"\"\n",
    "\n",
    "    def _add_leaf(value, class_name, indent):\n",
    "        val = \"\"\n",
    "        is_classification = isinstance(decision_tree, DecisionTreeClassifier)\n",
    "        if show_weights or not is_classification:\n",
    "            val = [\"{1:.{0}f}, \".format(decimals, v) for v in value]\n",
    "            val = \"[\" + \"\".join(val)[:-2] + \"]\"\n",
    "        if is_classification:\n",
    "            val += \" class: \" + str(class_name)\n",
    "        export_text.report += value_fmt.format(indent, \"\", val)\n",
    "\n",
    "    def print_tree_recurse(node, depth):\n",
    "        indent = (\"|\" + (\" \" * spacing)) * depth\n",
    "        indent = indent[:-spacing] + \"-\" * spacing\n",
    "        value = None\n",
    "        if tree_.n_outputs == 1:\n",
    "            value = tree_.value[node][0]\n",
    "        else:\n",
    "            value = tree_.value[node].T[0]\n",
    "        class_name = np.argmax(value)\n",
    "\n",
    "        if tree_.n_classes[0] != 1 and tree_.n_outputs == 1:\n",
    "            class_name = class_names[class_name]\n",
    "\n",
    "        if depth <= max_depth + 1:\n",
    "            info_fmt = \"\"\n",
    "            info_fmt_left = info_fmt\n",
    "            info_fmt_right = info_fmt\n",
    "\n",
    "            if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "                name = feature_names_[node]\n",
    "                threshold = tree_.threshold[node]\n",
    "                threshold = \"{1:.{0}f}\".format(decimals, threshold)\n",
    "                export_text.report += right_child_fmt.format(indent, name, threshold)\n",
    "                export_text.report += info_fmt_left\n",
    "                print_tree_recurse(tree_.children_left[node], depth + 1)\n",
    "\n",
    "                export_text.report += left_child_fmt.format(indent, name, threshold)\n",
    "                export_text.report += info_fmt_right\n",
    "                print_tree_recurse(tree_.children_right[node], depth + 1)\n",
    "            else:  # leaf\n",
    "                _add_leaf(value, class_name, indent)\n",
    "        else:\n",
    "            subtree_depth = _compute_depth(tree_, node)\n",
    "            if subtree_depth == 1:\n",
    "                _add_leaf(value, class_name, indent)\n",
    "            else:\n",
    "                trunc_report = \"truncated branch of depth %d\" % subtree_depth\n",
    "                export_text.report += truncation_fmt.format(indent, trunc_report)\n",
    "\n",
    "    print_tree_recurse(0, 1)\n",
    "    return export_text.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
