{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "from activation_functions import sigmoid\n",
    "from metrics import accuracy\n",
    "from BaseRegression import BaseRegression\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Decision tree is a powerful algorithm that can fit complex data and perform both classification, regression, and multioutput tasks.\n",
    "\n",
    "Advantage of Decision Trees:\n",
    "* Fairly intuitive and the decisions are easy to interpret. (white box model)\n",
    "* Feature scaling and centering is not necessary to obtain good results.\n",
    "\n",
    "Other noteworthy info:\n",
    "* Decision trees form the fundamental components of a RandomForest.\n",
    "* The CART algorithm (scikit-learn) produces only binary trees whereas ID3 for example allows nodes to have more than 2 children.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1060/1*H6thrs5CR_wdxQyMCwWawQ.png\" alt=\"Image of Decision tree\" style=\"background-color:white;\">\n",
    "\n",
    "### <span style=\"color:#217AB8\"> Making predictions</span> \n",
    "\n",
    "Starting at the root node and follow the conditions that apply to your current instance to the leaf. This will look like is the attribute x of your instance larger or smaller than 1. If yes follow the tree down the right path. If no go down the left. Once you reach a leaf node (aka does not have any child nodes) use this node's class to predict the class of your instance.\\\n",
    "It takes approximately $O(log_2(m))$ nodes to predict an instance's class. This is indipendent of the number of features so predictions are very fast even with large training sets.\n",
    "\n",
    "* *sample* how many training samples a node's condition applies to.\\\n",
    "* *value* how many training samples of each class a node applies to (eg. in node x: class a is represented 1, class b is represented 20 times)\n",
    "* *gini* measures the impurity of a node (pure when a node applies to only instances of one class)\n",
    "\n",
    "Gini impurity = $G_i = 1  - \\sum_{k=1}^n p_{i,k}^2$ where $p_{i,k}$ is the ratio of class k instances among the training instances in the ith node.\n",
    "\n",
    "### <span style=\"color:#217AB8\">CART classification and regression tree algorithm</span> \n",
    "A greedy algorithm meaning at every step from the beginning it tries to optimize the split rather than checking whether this improves impurity further down the line at lower levels.\\\n",
    "Therefore it does not guarantee an optimal solution.\\\n",
    "It is also an NP complete problem and requires O(exp(m)) time, making it hard to work with even small training sets. -> find reasonalbly good solutions.\n",
    "\n",
    "1. Split the training set into 2 using a single feature $k$ and a threshold $t_k$ (eg. petal length >= 1.3). Find the purest subsets for pairs (k, $t_k$) weighted by their size.\n",
    "\n",
    "&emsp;&emsp;&emsp;Minimize Cost function: $ J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$ where m is the number of instances in the subesets\n",
    "\n",
    "2. Continue this on the subsets recursively.\n",
    "3. Stop when max_depth is reached or if no split further reduces the gini impurity.\\\n",
    "Training complexity requires the comparison of all features (unless max_features is set) on all samples at each node.\\\n",
    "This brings the training compexity to $O(n * m * log(m))$\n",
    "\n",
    "### <span style=\"color:#217AB8\">Gini impurity or Entropy</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
