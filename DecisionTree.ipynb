{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "from activation_functions import sigmoid\n",
    "from metrics import accuracy\n",
    "from BaseRegression import BaseRegression\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Decision tree is a powerful algorithm that can fit complex data and perform both classification, regression, and multioutput tasks.\n",
    "\n",
    "Advantage of Decision Trees:\n",
    "* Make very few assumptions about the data.\n",
    "* Fairly intuitive and the decisions are easy to interpret. (white box model)\n",
    "* Feature scaling and centering is not necessary to obtain good results.\n",
    "\n",
    "Other noteworthy info:\n",
    "* Decision trees form the fundamental components of a RandomForest.\n",
    "* The CART algorithm (scikit-learn) produces only binary trees whereas ID3 for example allows nodes to have more than 2 children.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1060/1*H6thrs5CR_wdxQyMCwWawQ.png\" alt=\"Image of Decision tree\" style=\"background-color:white;\">\n",
    "\n",
    "### <span style=\"color:#217AB8\"> Making predictions</span> \n",
    "\n",
    "Starting at the root node and follow the conditions that apply to your current instance to the leaf. This will look like is the attribute x of your instance larger or smaller than 1. If yes follow the tree down the right path. If no go down the left. Once you reach a leaf node (aka does not have any child nodes) use this node's class to predict the class of your instance.\\\n",
    "It takes approximately $O(log_2(m))$ nodes to predict an instance's class. This is indipendent of the number of features so predictions are very fast even with large training sets.\n",
    "\n",
    "* *sample* how many training samples a node's condition applies to.\\\n",
    "* *value* how many training samples of each class a node applies to (eg. in node x: class a is represented 1, class b is represented 20 times)\n",
    "* *gini* measures the impurity of a node (pure when a node applies to only instances of one class)\n",
    "\n",
    "Gini impurity = $G_i = 1  - \\sum_{k=1}^n p_{i,k}^2$ where $p_{i,k}$ is the ratio of class k instances among the training instances in the ith node.\n",
    "\n",
    "### <span style=\"color:#217AB8\">CART classification and regression tree algorithm</span> \n",
    "A greedy algorithm meaning at every step from the beginning it tries to optimize the split rather than checking whether this improves impurity further down the line at lower levels.\\\n",
    "Therefore it does not guarantee an optimal solution.\\\n",
    "It is also an NP complete problem and requires O(exp(m)) time, making it hard to work with even small training sets. -> find reasonalbly good solutions.\n",
    "\n",
    "1. Split the training set into 2 using a single feature $k$ and a threshold $t_k$ (eg. petal length >= 1.3). Find the purest subsets for pairs (k, $t_k$) weighted by their size.\n",
    "\n",
    "&emsp;&emsp;&emsp;Minimize Cost function: $ J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$ where m is the number of instances in the subesets\n",
    "\n",
    "2. Continue this on the subsets recursively.\n",
    "3. Stop when max_depth is reached or if no split further reduces the gini impurity.\\\n",
    "Training complexity requires the comparison of all features (unless max_features is set) on all samples at each node.\\\n",
    "This brings the training compexity to $O(n * m * log(m))$\n",
    "\n",
    "If the tree is left unconstrained it will fit itself very closely to the training data and most likely overfitting.\\\n",
    "This is often described as a non-parametric model. In contrast, parametric models such as linear models have a pre-determined number of parameters, so their degrees of freedom are limited.\\\n",
    "(which in turn can lead to underfitting, especially when the data contains more complex patterns than the model is able to catch)\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:#217AB8\">Gini impurity or Entropy</span> \n",
    "Shannon's information theory: Entropy measures the average information content of a message:\\\n",
    "Entropy is 0 when all messages are identical.\\\n",
    "For example if in the case of decision tree the entropy is 0 then the node captures only one class.\n",
    "\n",
    "Entropy in the ith node: $H_i = \\sum_{k=1}^{n} p_{i,k}*log(p_{i,k})$ where $p_{i,k}\\not=0$\n",
    "\n",
    "Example:\n",
    "\n",
    "Should you use Gini or Entropy?\n",
    "According to O'reilly \"Hands on Machine learning\" they lead to similar trees.\\\n",
    "\"The gini index seems to be slightly faster to compute so it is a good default.\\\n",
    "However when they differ the Gini impurity tends to isolate the most frequent class  in its own branch of the tree, while entropy tends to produce slightly more balanced trees.\" (Sebastian Raschka's analysis)\n",
    "\n",
    "### <span style=\"color:#217AB8\">Regularization</span> \n",
    "For example:\\\n",
    "* restrict the maximum depth of the tree\n",
    "* set the minumum number of samples a node must have before it can split\n",
    "* set the minumum number of samples a leaf must have\n",
    "* set the minumum fraction of all training data that a leaf must represent\n",
    "* restrict the maximum number of leaf nodes that can be determined\n",
    "* restrict the maximum number of features considered for splitting at a node\n",
    "\n",
    "You could also train the tree without restrictions and then prune the tree after the training.\n",
    "For example:\n",
    "Prune a node if all of its children are leaves and it provides no statistically significant improvement of purity.\\\n",
    "Using the chi-sqaure test with null hypothesis that the node increases purity. If you want to reject the null hypothesis with 95% confidence then the p-value should be over 0.05. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
