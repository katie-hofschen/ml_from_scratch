{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "from metrics import accuracy\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality\n",
    "Many datasets have thousands or even millions of features per training instance - Using all of them will likely slow down your training.     \n",
    "Consider that:     \n",
    "Some of the features may not even add much information and could be discarded or ignored.     \n",
    "Other features may be so highly correlated that you might be able to merge them into one and lose little information.     \n",
    "   \n",
    "Furthermore, most people can readily understand 3 dimensions but begin to struggle with 4 dimensions, let alone thousands.    \n",
    "\n",
    "Other particularities of high dimensional data:\n",
    "* In a unit square (1x1) points are unlikely to be extreme along any dimension whereas in a high dimensional space (eg. 10.000 dims) most points lay along the border of their spaces hypercube.\n",
    "* high-dimensional datasets are at risk of being sparse, eg. the average distance between 2 points in a 3D unit square is 0.66, in a 10.000 dim space the average distance is around 408,25. \n",
    "\n",
    "In theory, you could increase the amount of training data to counter the sparseness of high dimensionality.\n",
    "Because however the number of training instances required for a given density grows exponentially with the number of features, this is unrealistic.\n",
    "As an example, for an average distance of 0.1 between points with 100 dimensions you would require more than the \"number of atoms in our observable universe\" (Aurelien GÃ©ron)\n",
    "\n",
    "## Main approaches to dimensionality reduction\n",
    "### Projection\n",
    "In real-world datasets most points lie within a lower-dimensional subspace meaning you may not need the full set of features to get a good approximation of your data.\n",
    "\n",
    "Projection essentially squasches higher dim points down onto a lower dimension. In some cases this works well because the higher dimensional data may already imitate the shape it would have in the lower dimension. In other cases such as the swiss toy roll dataset, different layers would be projected on top of each other, loosing valuable information in a lower dimension.\n",
    "\n",
    "\n",
    "### Manifold Learning\n",
    "Manifold - a shape in a lower dimensionality that has been twisted in a higher dimension. (TODO put images here for better visualization)\n",
    "(example of this the toy roll dataset)\n",
    "\n",
    "Manifold hypothesis: Most real-world high-dimensional data lie close to a much lower dimensional manifold. This assumptions is very often empirically observed.\n",
    "\n",
    "**As a note:**\\\n",
    "While dimensionality will always speed up training it will not always improve the predictions. In some cases the decision boundary will be less complex in higher dimesions whereas in others it will be so in smaller dimensions. That is why before using dimensionality reduction, a model should first be tested on the full set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: Principle Component Analysis\n",
    "\n",
    "PCA reduces dimesionality by identifying the hyperplane that is closest to the data and then projects the data onto this hyperplane."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
