{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "from metrics import accuracy\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality\n",
    "Many datasets have thousands or even millions of features per training instance - Using all of them will likely slow down your training.     \n",
    "Consider that:     \n",
    "Some of the features may not even add much information and could be discarded or ignored.     \n",
    "Other features may be so highly correlated that you might be able to merge them into one and lose little information.     \n",
    "   \n",
    "Furthermore, most people can readily understand 3 dimensions but begin to struggle with 4 dimensions, let alone thousands.    \n",
    "\n",
    "Other particularities of high dimensional data:\n",
    "* In a unit square (1x1) points are unlikely to be extreme along any dimension whereas in a high dimensional space (eg. 10.000 dims) most points lay along the border of their spaces hypercube.\n",
    "* high-dimensional datasets are at risk of being sparse, eg. the average distance between 2 points in a 3D unit square is 0.66, in a 10.000 dim space the average distance is around 408,25. \n",
    "\n",
    "In theory, you could increase the amount of training data to counter the sparseness of high dimensionality.\n",
    "Because however the number of training instances required for a given density grows exponentially with the number of features, this is unrealistic.\n",
    "As an example, for an average distance of 0.1 between points with 100 dimensions you would require more than the \"number of atoms in our observable universe\" (Aurelien GÃ©ron)\n",
    "\n",
    "## Main approaches to dimensionality reduction\n",
    "### Projection\n",
    "In real-world datasets most points lie within a lower-dimensional subspace meaning you may not need the full set of features to get a good approximation of your data.\n",
    "\n",
    "Projection essentially squasches higher dim points down onto a lower dimension. In some cases this works well because the higher dimensional data may already imitate the shape it would have in the lower dimension. In other cases such as the swiss toy roll dataset, different layers would be projected on top of each other, loosing valuable information in a lower dimension.\n",
    "\n",
    "\n",
    "### Manifold Learning\n",
    "Manifold - a shape in a lower dimensionality that has been twisted in a higher dimension. (TODO put images here for better visualization)\n",
    "(example of this the toy roll dataset)\n",
    "\n",
    "Manifold hypothesis: Most real-world high-dimensional data lie close to a much lower dimensional manifold. This assumptions is very often empirically observed.\n",
    "\n",
    "**As a note:**\\\n",
    "While dimensionality will always speed up training it will not always improve the predictions. In some cases the decision boundary will be less complex in higher dimesions whereas in others it will be so in smaller dimensions. That is why before using dimensionality reduction, a model should first be tested on the full set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: Principal Component Analysis\n",
    "\n",
    "PCA reduces dimesionality by identifying the hyperplane that is closest to the data and then projects the data onto this hyperplane.\n",
    "You can think of chosing the best hyperplane in 2 ways:\n",
    "* Preserving the data's variance as it is projected into a lower dimension. (In essence loosing as little information as possible.)\n",
    "* Minimizing the mean squared distance between the higher dimension points and their lower dimension counterparts.\n",
    "\n",
    "PCA identifies the axis that accounts for the most amount of variance, then orthoganally the axis that accounts for the second most remaining amount of variance, then a 3rd axis that is orthogonal and so on.\\\n",
    "These axes are represented by unit vectors - the ith unit vector is called the **ith Principal Component**.\\\n",
    "\n",
    "### How do you get the principal components?\n",
    "With a standard matrix factorization like SVD - Singular Value Decomposition.\\\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:894/1*XNWUlrQJXGeoCDqUMd0iUA.png\" height=\"200\" />\\\n",
    "[Read more about SVD here](https://mukundh-murthy.medium.com/a-beginners-guide-to-singular-value-decomposition-svd-97581e080c11)\n",
    "\n",
    "The singular values are are all of the principal components and can be sorted in a descending order to get the top-i components.\\\n",
    "\n",
    "**Note:** PCA supposes that the data is centered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
