{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "from metrics import accuracy\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Is a supervised learning algorithm based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.\n",
    "The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$.\n",
    "\n",
    "They work quite well, can be very fast and require a small amount of training data to estimate the necessary parameters. They have been famously used in document classification and spam filtering. However, it is known to be a bad estimator, so the probability outputs from are not to be taken too seriously.\n",
    "\n",
    "Bayes Theorem:  \n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) * P(A)} {P(B)} $   \n",
    "\n",
    "The probability of A given B.    \n",
    "\n",
    "In case of our target and data:    \n",
    "\n",
    "$P(y|X)$   - Posterior Probability    \n",
    "$P(y)$     - Prior probability of y    \n",
    "$P(x_i|y)$ - Class conditional probability\n",
    "\n",
    "Using the chain rule we get:\n",
    "$P(y|X) = \\frac{P(y)  *  \\prod_i{P(x_i|y)}} {P(X)} $\n",
    "\n",
    "$argmax(P(y|X))$ select the class with the highest probability.\n",
    "\n",
    "Finally: $argmax( log(P(x_1|y)) + log(P(x_2|y)) + ... + log(P(y)) )$\n",
    "\n",
    "\n",
    "\n",
    "Using the Gaussion distribution:\n",
    "\n",
    "$P(x_i|y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_y^2}} * exp(-\\frac{(x_i-\\mu_y)^2}{2 \\sigma_y^2}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        # p_y = frequency\n",
    "        # p_class_cond = P(xi|y)\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        self.n_classes = len(self._classes)\n",
    "        \n",
    "        # Initialize vectors needed for calculations\n",
    "        self._mean = np.zeros((self.n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((self.n_classes, n_features), dtype=np.float64)\n",
    "        for clss in self._classes:\n",
    "            self._mean[clss,:] = X[y==clss].mean(axis=0)\n",
    "            self._var[clss,:] = X[y==clss].var(axis=0)\n",
    "\n",
    "        # Prior P(y) = frequencies of classes\n",
    "        self._priors = [np.sum([y==clss]) / n_samples for clss in self._classes]\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict(x) for x in X]\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # chose class with highers probability from posteriors\n",
    "        # Posteriors = P(y|x) for every y\n",
    "        posteriors = [np.log(self._priors[id]) + np.sum(np.log(self._gaussian_pd(id,x))) for id in range(self.n_classes)]\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "        \n",
    "\n",
    "    def _gaussian_pd(self, cid, x):\n",
    "        mean = self._mean[cid]\n",
    "        var = self._var[cid]\n",
    "        numerator = np.exp(- np.square(x-mean) / (2 * var))\n",
    "        denominator = np.sqrt(1* np.pi * var)\n",
    "        return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive = NaiveBayes()\n",
    "naive.fit(X_train, y_train)\n",
    "predicted = naive.predict(X_test)\n",
    "\n",
    "accuracy(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
